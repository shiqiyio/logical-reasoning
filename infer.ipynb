{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "396e8401-650e-4130-9923-c719c139b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_prompt(instruction, input):\n",
    "    # 提取选项部分\n",
    "    options = []\n",
    "    for line in input.split('; '):\n",
    "        if line.startswith('A:') or line.startswith('B:') or line.startswith('C:') or line.startswith('D:'):\n",
    "            options.append(line.split(':', 1)[1])\n",
    "\n",
    "    # 构建选项字符串\n",
    "    options_str = '\\n'.join(f\"{'ABCD'[i]}. {o}\" for i, o in enumerate(options))\n",
    "\n",
    "    # 构建prompt\n",
    "    prompt = f\"\"\"你是一个逻辑推理专家，擅长解决逻辑推理问题。以下是一个逻辑推理的题目，形式为单项选择题。所有的问题都是（close-world assumption）闭世界假设，即未观测事实都为假。请逐步分析问题并在最后一行输出答案，最后一行的格式为\"A\"。题目如下：\n",
    "\n",
    "### 题目:\n",
    "{instruction}\n",
    "\n",
    "### 问题:\n",
    "{input}\n",
    "{options_str}\n",
    "\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def read_and_process_json_file(file_path):\n",
    "    prompts = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for item in data:\n",
    "        instruction = item[\"instruction\"]\n",
    "        input = item[\"input\"]\n",
    "        prompt = get_prompt(instruction, input)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    return prompts\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 指定输入文件路径\n",
    "#     input_file_path = './dataset/output_test.json'\n",
    "\n",
    "#     # 读取并处理JSON文件\n",
    "#     prompts = read_and_process_json_file(input_file_path)\n",
    "#     print(prompts[:10])\n",
    "\n",
    "#     # # 打印生成的prompts\n",
    "#     # for i, prompt in enumerate(prompts, start=1):\n",
    "#     #     print(f\"Prompt {i}:{prompt}\")\n",
    "#     #     if i==3:\n",
    "#     #         break\n",
    "#     print(len(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7751b440-9a94-4e9c-915f-be8877a8f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d336894f281e46d9a39fa603c05ffa28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14/1328 [01:08<1:33:13,  4.26s/it]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "mode_path = '/share/new_models/qwen/Qwen2-7B-Instruct/'\n",
    "# mode_path = '/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b'\n",
    "lora_path = './output/Qwen2_instruct_lora/checkpoint-100' # 这里改称你的 lora 输出对应 checkpoint 地址\n",
    "# mode_path = './merge/'\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_path, trust_remote_code=True)\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(mode_path, device_map=\"auto\",torch_dtype=torch.bfloat16, trust_remote_code=True).eval()\n",
    "\n",
    "# 加载lora权重\n",
    "# model = PeftModel.from_pretrained(model, model_id=lora_path)\n",
    "input_file_path = './dataset/output_test.json'\n",
    "\n",
    "    # 读取并处理JSON文件\n",
    "prompts = read_and_process_json_file(input_file_path)\n",
    "\n",
    "\n",
    "\n",
    "# prompt = \"题目如下：有一个列表，找出该列表的最后一个元素。\\n\\n下列选项中哪个是列表 `[a, b, c, d]` 的最后一个元素？A: a; B: b; C: c; D: d\"\n",
    "answer_list=[]\n",
    "for prompt in tqdm(prompts):       \n",
    "    inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}],\n",
    "                                           add_generation_prompt=True,\n",
    "                                           tokenize=True,\n",
    "                                           return_tensors=\"pt\",\n",
    "                                           return_dict=True\n",
    "                                           ).to('cuda')\n",
    "    \n",
    "    \n",
    "    gen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer_list.append(answer)\n",
    "    # print(answer_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1596826d-4088-429a-8460-3f58bf4cf56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] gemm_config.in is not found; using default GEMM algo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1328 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from lmdeploy import pipeline, GenerationConfig, PytorchEngineConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lmdeploy import pipeline, GenerationConfig, TurbomindEngineConfig\n",
    "import nest_asyncio\n",
    " \n",
    "nest_asyncio.apply()\n",
    "backend_config = TurbomindEngineConfig(tp=1)\n",
    "\n",
    "# backend_config = PytorchEngineConfig(session_len=2048,\n",
    "                                     # adapters=dict(lora_name_1='chenchi/lora-chatglm2-6b-guodegang'))\n",
    "gen_config = GenerationConfig(top_p=0.2,\n",
    "                              top_k=1,\n",
    "                              temperature=0.2,\n",
    "                              max_new_tokens=1024)\n",
    "pipe = pipeline('/root/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b',backend_config=backend_config)\n",
    "input_file_path = './dataset/output_test.json'\n",
    "\n",
    "    # 读取并处理JSON文件\n",
    "prompts = read_and_process_json_file(input_file_path)\n",
    "\n",
    "\n",
    "\n",
    "# prompt = \"题目如下：有一个列表，找出该列表的最后一个元素。\\n\\n下列选项中哪个是列表 `[a, b, c, d]` 的最后一个元素？A: a; B: b; C: c; D: d\"\n",
    "answer_list=[]\n",
    "for prompt in tqdm(prompts):\n",
    "\n",
    "    response = pipe(prompt, gen_config=gen_config)\n",
    "    answer_list.append(response)\n",
    "    # print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead5f544-a636-4e22-a470-69e0ddcdbe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def process_json(json_file_path, answer_list, output_file_path):\n",
    "    # 读取 JSON 文件\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # 初始化输出数据结构\n",
    "    output_data = []\n",
    "\n",
    "    # 初始化 round_id\n",
    "    round_id = 0\n",
    "    current_instruction = \"\"\n",
    "\n",
    "    # 处理每一项数据\n",
    "    for item in data:\n",
    "        # 如果 instruction 发生变化，增加 round_id\n",
    "        if current_instruction != item['instruction']:\n",
    "            round_id += 1\n",
    "            current_instruction = item['instruction']\n",
    "            current_item = {\n",
    "                'id': f'round1_test_data_{round_id:03d}',\n",
    "                'questions': []\n",
    "            }\n",
    "            output_data.append(current_item)\n",
    "        else:\n",
    "            current_item = output_data[-1]\n",
    "\n",
    "        # 解析 input 字段中的选择题编号\n",
    "        question_number = int(item['input'].split('选择题 ')[1].split(':')[0])\n",
    "\n",
    "        # 获取对应答案\n",
    "        answer = answer_list.pop(0)  # 使用并移除第一个答案\n",
    "\n",
    "        # 添加问题及答案到 questions 列表\n",
    "        current_item['questions'].append({'answer': answer})\n",
    "\n",
    "    # 写入 JSONL 文件\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "        for item in output_data:\n",
    "            jsonl_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ee8e2-63fa-448e-b043-25cd5ae18323",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(answer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c268e0d6-ab21-42af-a416-bb386ea89a76",
   "metadata": {},
   "outputs": [],
   "source": [
    " input_file_path = './dataset/output_test.json'\n",
    "\n",
    "# 指定答案列表\n",
    "answers = answer_list\n",
    "\n",
    "# 指定输出文件路径\n",
    "output_file_path = './dataset/submit.jsonl'\n",
    "\n",
    "# 处理 JSON 文件并输出到 JSONL 文件\n",
    "process_json(input_file_path, answers, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29bc5c7-91f2-4688-ac2c-08aa4585bece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_list_to_file(file_path, data_list):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for item in data_list:\n",
    "            file.write(f\"{item}\\n\")\n",
    "\n",
    "# 假设这是你的列表\n",
    "options =  answer_list\n",
    "\n",
    "# 文件路径\n",
    "file_path = 'a.txt'\n",
    "\n",
    "# 写入文件\n",
    "write_list_to_file(file_path, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02059149-7a2a-46e8-a785-3108643c35b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtuner0121",
   "language": "python",
   "name": "xtuner0121"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
